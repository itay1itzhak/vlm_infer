model:
  name: "llava_7B"
  type: "hf"
  checkpoint: "llava-hf/llava-1.5-7b-hf"
  temperature: 0.0  # Use greedy decoding
  max_new_tokens: 512
  is_chat_model: true

dataset:
  name: "vtom"
  data_dir: "data/vtom"
  version: "take_4.1"
  split: "test"
  inference_type: "image_story"
  image_size: [224, 224]

inference:
  batch_size: 1
  num_workers: 4
  save_images: true
  verbose: true 