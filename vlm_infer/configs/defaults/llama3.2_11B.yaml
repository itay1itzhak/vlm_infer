model:
  name: "llama3.2_11B"
  type: "hf"
  checkpoint: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  is_chat_model: true
  prompt_template: "{question}"
  torch_dtype: "bfloat16"
  device_map: "auto"
  model_class: "MllamaForConditionalGeneration" 