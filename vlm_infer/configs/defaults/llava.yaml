model:
  type: "hf"
  checkpoint: "llava-hf/llava-1.5-7b-hf"
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  is_chat_model: true
  prompt_template: "Question: {question}\nAnswer:"
  torch_dtype: "float16"
  device_map: "auto"
  model_class: "LlavaForConditionalGeneration" 