model:
  type: "hf"
  checkpoint: "meta-llama/Llama-3.2-11B-Vision-Instruct"
  max_new_tokens: 512
  temperature: 0.7
  top_p: 0.9
  is_chat_model: true
  torch_dtype: "bfloat16"
  device_map: "auto"
  model_class: "MllamaForConditionalGeneration"

dataset:
  name: "vtom"
  split: "test"
  image_size: [224, 224]
  inference_type: "standard"

inference:
  batch_size: 1
  num_workers: 4
  save_images: false
  verbose: true 